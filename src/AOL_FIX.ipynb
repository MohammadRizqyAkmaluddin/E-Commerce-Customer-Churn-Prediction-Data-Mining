{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1906126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Train shape: (3066, 29), Test shape: (767, 29)\n",
      "Distribusi y_train:\n",
      "Churn\n",
      "0    0.804305\n",
      "1    0.195695\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv(\"../output/X_train.csv\")\n",
    "X_test = pd.read_csv(\"../output/X_test.csv\")\n",
    "y_train = pd.read_csv(\"../output/y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"../output/y_test.csv\").squeeze()\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(\"Distribusi y_train:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85279a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan pipeline tiap model\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC(probability=True, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA()),          # PCA valid\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "lgbm_model = LGBMClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93761450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param grids \n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model__C\": [0.5, 1, 2],\n",
    "        \"model__solver\": [\"lbfgs\"],\n",
    "        \"model__penalty\": [\"l2\"]\n",
    "    },\n",
    "\n",
    "    \"SVM\": {\n",
    "        \"model__C\": [0.1, 0.5, 1],\n",
    "        \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "        \"model__gamma\": [\"scale\"]\n",
    "    },\n",
    "\n",
    "    \"KNN\": {\n",
    "        \"model__n_neighbors\": [7, 11, 15, 21],\n",
    "        \"model__weights\": [\"uniform\"],\n",
    "        \"pca__n_components\": [5, 7, 10]\n",
    "    },\n",
    "\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [4, 6, 8],\n",
    "        \"min_samples_split\": [20, 40, 60],\n",
    "        \"min_samples_leaf\": [8, 12, 16],   # PENTING\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"bootstrap\": [True],\n",
    "        \"max_samples\": [0.6, 0.8]          # NEW: bagging lebih kecil → lebih general\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [3, 4],\n",
    "        \"learning_rate\": [0.05],\n",
    "        \"subsample\": [0.8],\n",
    "        \"colsample_bytree\": [0.8]\n",
    "    },\n",
    "\n",
    "    \"LightGBM\": {\n",
    "        \"max_depth\": [4, 6],        # shallow saja – mencegah overfit\n",
    "        \"num_leaves\": [8, 16],      # selalu ≤ 2^(max_depth)\n",
    "        \"min_child_samples\": [20, 40],   # regularisasi paling kuat\n",
    "        \"learning_rate\": [0.03],    # tetap stabil\n",
    "        \"n_estimators\": [200],      # cukup, tidak berlebihan\n",
    "        \"subsample\": [0.7],         # bagging moderate\n",
    "        \"colsample_bytree\": [0.7],  # feature sampling moderate\n",
    "        \"reg_lambda\": [1],          # L2 penalty untuk stabilisasi\n",
    "    }\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f59e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def tune_model_once(model, name, X_train, y_train):\n",
    "    if name not in param_grids:\n",
    "        print(f\"Tidak ada param_grid untuk {name}, pakai default model.\")\n",
    "        return model\n",
    "\n",
    "    print(f\"\\n Tuning {name} ...\")\n",
    "    param_grid = param_grids[name]\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"✅ Best params for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21ba862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi dengan CV 5-Fold\n",
    "def evaluate_model_cv(model, name, X, y):\n",
    "    acc_scores, prec_scores, rec_scores, f1_scores, roc_scores = [], [], [], [], []\n",
    "\n",
    "    for train_idx, test_idx in cv_outer.split(X, y):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_test_fold)\n",
    "\n",
    "        # ROC AUC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_score = model.predict_proba(X_test_fold)[:, 1]\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_score = model.decision_function(X_test_fold)\n",
    "        else:\n",
    "            y_score = y_pred\n",
    "\n",
    "        acc_scores.append(accuracy_score(y_test_fold, y_pred))\n",
    "        prec_scores.append(precision_score(y_test_fold, y_pred))\n",
    "        rec_scores.append(recall_score(y_test_fold, y_pred))\n",
    "        f1_scores.append(f1_score(y_test_fold, y_pred))\n",
    "        roc_scores.append(roc_auc_score(y_test_fold, y_score))\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": np.mean(acc_scores),\n",
    "        \"Precision\": np.mean(prec_scores),\n",
    "        \"Recall\": np.mean(rec_scores),\n",
    "        \"F1 Score\": np.mean(f1_scores),\n",
    "        \"ROC AUC\": np.mean(roc_scores)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== {name} (CV 5-Fold) ===\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != \"Model\":\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020c453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuning Logistic Regression ...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "✅ Best params for Logistic Regression: {'model__C': 1, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n",
      "\n",
      " Tuning Random Forest ...\n",
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "✅ Best params for Random Forest: {'bootstrap': True, 'max_depth': 8, 'max_features': 'sqrt', 'max_samples': 0.8, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 400}\n",
      "\n",
      " Tuning KNN ...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "✅ Best params for KNN: {'model__n_neighbors': 7, 'model__weights': 'uniform', 'pca__n_components': 10}\n",
      "\n",
      " Tuning SVM ...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "✅ Best params for SVM: {'model__C': 1, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\n",
      "\n",
      " Tuning XGBoost ...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best params for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\n",
      " Tuning LightGBM ...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 600, number of negative: 2466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 417\n",
      "[LightGBM] [Info] Number of data points in the train set: 3066, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195695 -> initscore=-1.413423\n",
      "[LightGBM] [Info] Start training from score -1.413423\n",
      "✅ Best params for LightGBM: {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_samples': 20, 'n_estimators': 200, 'num_leaves': 16, 'reg_lambda': 1, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# Jalankan tuning\n",
    "models = {\n",
    "    \"Logistic Regression\": logreg_pipeline,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"KNN\": knn_pipeline,\n",
    "    \"SVM\": svm_pipeline,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"LightGBM\": lgbm_model\n",
    "}\n",
    "tuned_models = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    best_model = tune_model_once(model, name, X_train, y_train)\n",
    "    tuned_models.append((best_model, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8732522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression (CV 5-Fold) ===\n",
      "Accuracy: 0.8764\n",
      "Precision: 0.7510\n",
      "Recall: 0.5550\n",
      "F1 Score: 0.6361\n",
      "ROC AUC: 0.8928\n",
      "\n",
      "=== Random Forest (CV 5-Fold) ===\n",
      "Accuracy: 0.8780\n",
      "Precision: 0.8536\n",
      "Recall: 0.4550\n",
      "F1 Score: 0.5926\n",
      "ROC AUC: 0.9323\n",
      "\n",
      "=== KNN (CV 5-Fold) ===\n",
      "Accuracy: 0.8288\n",
      "Precision: 0.6020\n",
      "Recall: 0.3633\n",
      "F1 Score: 0.4530\n",
      "ROC AUC: 0.7987\n",
      "\n",
      "=== SVM (CV 5-Fold) ===\n",
      "Accuracy: 0.8832\n",
      "Precision: 0.6521\n",
      "Recall: 0.8700\n",
      "F1 Score: 0.7448\n",
      "ROC AUC: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:45:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost (CV 5-Fold) ===\n",
      "Accuracy: 0.9041\n",
      "Precision: 0.8185\n",
      "Recall: 0.6550\n",
      "F1 Score: 0.7273\n",
      "ROC AUC: 0.9418\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 480, number of negative: 1972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 417\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195759 -> initscore=-1.413017\n",
      "[LightGBM] [Info] Start training from score -1.413017\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 480, number of negative: 1973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 417\n",
      "[LightGBM] [Info] Number of data points in the train set: 2453, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195679 -> initscore=-1.413524\n",
      "[LightGBM] [Info] Start training from score -1.413524\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 480, number of negative: 1973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 415\n",
      "[LightGBM] [Info] Number of data points in the train set: 2453, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195679 -> initscore=-1.413524\n",
      "[LightGBM] [Info] Start training from score -1.413524\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 480, number of negative: 1973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 416\n",
      "[LightGBM] [Info] Number of data points in the train set: 2453, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195679 -> initscore=-1.413524\n",
      "[LightGBM] [Info] Start training from score -1.413524\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 480, number of negative: 1973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 417\n",
      "[LightGBM] [Info] Number of data points in the train set: 2453, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195679 -> initscore=-1.413524\n",
      "[LightGBM] [Info] Start training from score -1.413524\n",
      "\n",
      "=== LightGBM (CV 5-Fold) ===\n",
      "Accuracy: 0.9155\n",
      "Precision: 0.8361\n",
      "Recall: 0.7067\n",
      "F1 Score: 0.7656\n",
      "ROC AUC: 0.9534\n",
      "\n",
      " Hasil Akhir Setelah Hyperparameter Tuning:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score   ROC AUC\n",
      "0             LightGBM  0.915521   0.836055  0.706667  0.765645  0.953425\n",
      "1                  SVM  0.883229   0.652144  0.870000  0.744781  0.944130\n",
      "2              XGBoost  0.904107   0.818457  0.655000  0.727320  0.941813\n",
      "3  Logistic Regression  0.876384   0.750986  0.555000  0.636130  0.892835\n",
      "4        Random Forest  0.878013   0.853586  0.455000  0.592581  0.932291\n",
      "5                  KNN  0.828759   0.601966  0.363333  0.452957  0.798656\n",
      "\n",
      " Model terbaik berhasil disimpan\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi hasil CV\n",
    "results_cv = []\n",
    "for model, name in tuned_models:\n",
    "    result = evaluate_model_cv(model, name, X_train, y_train)\n",
    "    results_cv.append(result)\n",
    "\n",
    "results_cv_df = pd.DataFrame(results_cv).sort_values(by=\"F1 Score\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n Hasil Akhir Setelah Hyperparameter Tuning:\")\n",
    "print(results_cv_df)\n",
    "\n",
    "joblib.dump(tuned_models[0][0], \"best_model.pkl\")\n",
    "print(\"\\n Model terbaik berhasil disimpan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ad3f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Check Overfitting: Logistic Regression ===\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.8819 | F1: 0.6499\n",
      "Test Accuracy:  0.8774 | F1: 0.6179\n",
      "Selisih Accuracy: 0.0045\n",
      "Selisih F1:       0.0320\n",
      "Tidak terdeteksi overfitting signifikan.\n",
      "\n",
      "=== Check Overfitting: Random Forest ===\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.9136 | F1: 0.7282\n",
      "Test Accuracy:  0.8944 | F1: 0.6368\n",
      "Selisih Accuracy: 0.0192\n",
      "Selisih F1:       0.0914\n",
      "Tidak terdeteksi overfitting signifikan.\n",
      "\n",
      "=== Check Overfitting: KNN ===\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.8751 | F1: 0.6112\n",
      "Test Accuracy:  0.8422 | F1: 0.4762\n",
      "Selisih Accuracy: 0.0328\n",
      "Selisih F1:       0.1350\n",
      "⚠️  Model kemungkinan overfitting (di atas ambang toleransi)!\n",
      "\n",
      "=== Check Overfitting: SVM ===\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.9220 | F1: 0.8299\n",
      "Test Accuracy:  0.8918 | F1: 0.7566\n",
      "Selisih Accuracy: 0.0303\n",
      "Selisih F1:       0.0733\n",
      "Tidak terdeteksi overfitting signifikan.\n",
      "\n",
      "=== Check Overfitting: XGBoost ===\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.9335 | F1: 0.8149\n",
      "Test Accuracy:  0.9113 | F1: 0.7344\n",
      "Selisih Accuracy: 0.0221\n",
      "Selisih F1:       0.0805\n",
      "Tidak terdeteksi overfitting signifikan.\n",
      "\n",
      "=== Check Overfitting: LightGBM ===\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 600, number of negative: 2466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 417\n",
      "[LightGBM] [Info] Number of data points in the train set: 3066, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.195695 -> initscore=-1.413423\n",
      "[LightGBM] [Info] Start training from score -1.413423\n",
      "Jumlah data: train=3066, test=767\n",
      "Train Accuracy: 0.9459 | F1: 0.8515\n",
      "Test Accuracy:  0.9205 | F1: 0.7645\n",
      "Selisih Accuracy: 0.0254\n",
      "Selisih F1:       0.0870\n",
      "Tidak terdeteksi overfitting signifikan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MUHAMMAD RIZQY\\Documents\\Projects\\Data Mining\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:46:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# Check overfitting\n",
    "# =============================\n",
    "def check_overfitting(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    train_f1 = f1_score(y_train, y_pred_train)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    # Hitung ukuran data\n",
    "    n_train = len(X_train)\n",
    "    n_test = len(X_test)\n",
    "    print(f\"Jumlah data: train={n_train}, test={n_test}\")\n",
    "\n",
    "    accuracy_threshold = 0.07\n",
    "    f1_threshold = 0.12\n",
    "\n",
    "    # Tampilkan metrik\n",
    "    print(f\"Train Accuracy: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"Test Accuracy:  {test_acc:.4f} | F1: {test_f1:.4f}\")\n",
    "    print(f\"Selisih Accuracy: {abs(train_acc - test_acc):.4f}\")\n",
    "    print(f\"Selisih F1:       {abs(train_f1 - test_f1):.4f}\")\n",
    "\n",
    "    # Evaluasi overfitting\n",
    "    if abs(train_acc - test_acc) > accuracy_threshold or abs(train_f1 - test_f1) > f1_threshold:\n",
    "        print(\"⚠️  Model kemungkinan overfitting (di atas ambang toleransi)!\")\n",
    "    else:\n",
    "        print(\"Tidak terdeteksi overfitting signifikan.\")\n",
    "\n",
    "\n",
    "for model, name in tuned_models:\n",
    "    print(f\"\\n=== Check Overfitting: {name} ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "    check_overfitting(model, X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
